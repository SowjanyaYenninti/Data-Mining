{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNEu4kqipo3r"
      },
      "source": [
        "\n",
        "<br>\n",
        "Created on 10-Sep-2021<br>\n",
        "@author: sowjanya y<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A74SJianpo3v"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from itertools import permutations\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from itertools import combinations\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQhyxb5Ipo3w"
      },
      "outputs": [],
      "source": [
        "def EXP_PRE():\n",
        "    df = pd.read_csv(\"Employee_skills_traits.csv\")\n",
        "    \n",
        "    #removing the ID column and writing remaining columns to a separate dataframe\n",
        "    order = [1,2,3,4,5,6,7,8,9,10,11,12,13,] # setting column's \n",
        "    df_d = df[[df.columns[i] for i in order]]\n",
        "    df_d.info() # to get information on the data\n",
        "    df_d.describe().to_csv('Describe_dataframe.csv', index = True) #to get the statistics of the columns of dataframe\n",
        "    #print(\"Number of rows and columns after removing the ID column :\")\n",
        "    #print( )\n",
        "    #print(df_d)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql1w2i8Qpo3x"
      },
      "source": [
        "rop the duplicate rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkoBQg-Kpo3x"
      },
      "outputs": [],
      "source": [
        "    df_d = df_d.drop_duplicates()\n",
        "    #print(\"after removing duplicates: \")\n",
        "    #print( )\n",
        "    #print(df_d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCoNbBNhpo3y"
      },
      "outputs": [],
      "source": [
        " # IQR    \n",
        "    Q1 = df_d.quantile(0.25)\n",
        "    Q3 = df_d.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    #print(IQR)\n",
        "\n",
        "    #removing noisy data \n",
        "    df_d = df_d[~((df_d < (Q1 - 1.5 * IQR)) |(df_d > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "    \n",
        "    df_d.describe().to_csv('Describe_remove_noisy.csv', index = True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y36YQTn4po3y"
      },
      "source": [
        "here Features - Age, Employment period and Time in current department columns <br>\n",
        "are taken for normalization to handle values with varying magnitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OrevyOYpo3z"
      },
      "outputs": [],
      "source": [
        "    x = ['Employment period ','Time in current department ', 'Age ']\n",
        "    #print (\"\\nOriginal data values : \\n\",  x)\n",
        "    \"\"\" MIN MAX SCALER \"\"\"\n",
        "  \n",
        "    min_max_scaler = preprocessing.MinMaxScaler(feature_range =(0, 1))\n",
        "  \n",
        "    # Scaled feature\n",
        "    df_d[x] = min_max_scaler.fit_transform(df_d[x])\n",
        "    #print (\"\\nAfter min max Scaling of the columns: \\n\", df_d[x])\n",
        "    \n",
        "   \n",
        "    \n",
        "    #split data into train data and test data\n",
        "    train, test = train_test_split(df_d, test_size=0.2)\n",
        "    train.describe().to_csv('Decribe_train.csv', index = True) \n",
        "    test.describe().to_csv('Describe_test.csv', index = True) \n",
        "    return train, test\n",
        "    \n",
        "#This function is used to get the frequent Pattern sets\n",
        "def FREQ_PATTERN(data, min_support, max_length):\n",
        "    \n",
        "    si = data.shape[0] # get the number of rows\n",
        "    # min_suppport is minimum threshold support.\n",
        "    # where max_ Length is maximum number of Columns to be included in Pattern set.\n",
        "\n",
        "    # Step 1:\n",
        "# Creating a dictionary to stored support of an Pattern set.\n",
        "   \n",
        "    support = {}\n",
        "    L = set(train.columns)\n",
        "    #print(\"L is:\", L)\n",
        "\n",
        "    # Step 2:\n",
        "    # Generating combination of Columns with L and ith iteration\n",
        "    for i in range(1, max_length+1): \n",
        "        c = list(combinations(L,i))\n",
        "        #print(\"Combi is:\", c)\n",
        "\n",
        "        # Reset \"L\" for next (i+l)th iteration\n",
        "        L =set()\n",
        "        # Step 3:\n",
        "        #iterate through each item in \"c\"\n",
        "        for j in list(c):\n",
        "            sup = data.loc[:,j].product(axis=1).sum()/len(data.index)\n",
        "            #print(\"sup is:\", sup)\n",
        "            if sup > min_support: \n",
        "                support[j]= sup\n",
        "                # Appending frequent Pattern set in List \"L \", already reset List \"L\"\n",
        "                L = set(set(L) |  set(j))\n",
        "                #print(\"L after supp:\",L)\n",
        "        \n",
        "        #i = i+1\n",
        "# Step 4: data frame with columns \"Patterns\", 'support'\n",
        "    result = pd.DataFrame(list(support.items()), columns = [\"Patterns\" , \"Support\", ])\n",
        "    if si == 798:\n",
        "        if min_support == 0.25:\n",
        "            result.to_csv('Freq_Columns_train_0.25S.csv', index = True)\n",
        "        else:\n",
        "            result.to_csv('Freq_Columns_train_0.30S.csv', index = True)\n",
        "    else:\n",
        "        if min_support == 0.21:\n",
        "            result.to_csv('Freq_Columns_test_0.21S.csv', index = True)\n",
        "        else:\n",
        "            result.to_csv('Freq_Columns_test_0.35S.csv', index = True)\n",
        "    #print(result)\n",
        "    return(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m_AoWIrpo30"
      },
      "source": [
        "his function is used to get the association rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBMSRFwdpo31"
      },
      "outputs": [],
      "source": [
        "def ASSOCIATION_RULE(df, min_threshold, min_support, fl):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRcrCpOCpo31"
      },
      "source": [
        "STEP 1:<br>\n",
        "reating required variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJggUa3jpo32"
      },
      "outputs": [],
      "source": [
        "    support = pd.Series(df.Support.values, index=df.Patterns).to_dict() \n",
        "    data = []\n",
        "    L= df.Patterns.values\n",
        "    #print(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Ugo3Yrpo32"
      },
      "source": [
        "Step 2:<br>\n",
        "enerating rule using permutation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KhgHFoppo32"
      },
      "outputs": [],
      "source": [
        "    p = list(permutations(L, 2))\n",
        "    #print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YovCbUJpo32"
      },
      "source": [
        "Iterating through each rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REVO865Hpo33"
      },
      "outputs": [],
      "source": [
        "    for i in p:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62mTtF5Qpo33"
      },
      "source": [
        "If LHS(Antecedent)of rule is subset of RHS then valid rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkUTIkZPpo33"
      },
      "outputs": [],
      "source": [
        "        if set(i[0]).issubset(i[1]):\n",
        "            conf = support[i[1]]/support[i[0]]\n",
        "            if conf > min_threshold :\n",
        "                j = i [1][not i[1].index(i[0][0])]\n",
        "                lift = support[i[1]]/(support[i[0]]* support[(j,)])\n",
        "                data.append ([i[0], (j,), support[i[0]], support[(j,)], support[i[1]], conf, lift ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DREKYh-0po33"
      },
      "source": [
        "STEP 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOssXYIFpo34"
      },
      "outputs": [],
      "source": [
        "    result = pd.DataFrame(data, columns = [\"antecedents\", \"consequents\", \"antecedent support\", \n",
        "                                            \"consequent support\", \"support\", \"confidence\", \"Lift\" ])\n",
        "    \n",
        "    if fl == 'train':\n",
        "        if min_support == 0.25:\n",
        "            result.to_csv('Rules_train_0.25S.csv', index = True)\n",
        "        else:\n",
        "            result.to_csv('Rules_train_0.30S.csv', index = True)\n",
        "    else:\n",
        "        if min_support == 0.21:\n",
        "            result.to_csv('Rules_test_0.21S.csv', index = True)\n",
        "        else:\n",
        "            result.to_csv('Rules_test_0.35S.csv', index = True)\n",
        "    #print(result)\n",
        "    return (result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOmMwhmLpo34"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    train, test = EXP_PRE()\n",
        "    #print(train.shape)\n",
        "    #print(test.shape)\n",
        "    \n",
        "#finding frequent Pattern sets for train data when support is 0.25\n",
        "    freq_patternset_train = FREQ_PATTERN(train, 0.25, 4)\n",
        "    freq_patternset_train.sort_values(by = 'Support', ascending = False)\n",
        "    # Rule with minimum confidence 50% for train data\n",
        "    my_rule = ASSOCIATION_RULE(freq_patternset_train, 0.5, 0.25, 'train')\n",
        "    \n",
        "#finding frequent Pattern sets for train data when support is 0.3\n",
        "    freq_patternset_train = FREQ_PATTERN(train, 0.3, 4)\n",
        "    freq_patternset_train.sort_values(by = 'Support', ascending = False)\n",
        "    # Rule with minimum confidence 50% for train data\n",
        "    my_rule = ASSOCIATION_RULE(freq_patternset_train, 0.5, 0.3, 'train')\n",
        "    \n",
        "    #finding frequent Pattern sets for test data when support is 0.21\n",
        "    freq_patternset_test= FREQ_PATTERN(test, 0.21, 4)\n",
        "    freq_patternset_test.sort_values(by = 'Support', ascending = False)\n",
        "    # Rule with minimum confidence 50% for test data\n",
        "    my_rule = ASSOCIATION_RULE(freq_patternset_test, 0.5, 0.21, 'test')\n",
        "    \n",
        "#finding frequent Pattern sets for test data when support is 0.35\n",
        "    freq_patternset_test= FREQ_PATTERN(test, 0.35, 4)\n",
        "    freq_patternset_test.sort_values(by = 'Support', ascending = False)\n",
        "    # Rule with minimum confidence 50% for test data\n",
        "    my_rule = ASSOCIATION_RULE(freq_patternset_test, 0.5, 0.35, 'test')\n",
        "    \n",
        "    print(\"KDD process successful\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Skill.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}